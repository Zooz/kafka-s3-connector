
# The service uses a loader module to bind the BuffersManager, HealthChecker and KafkaMessage implementations.
# It gets the name of the module from the LOADING_MODULE environment variable.
# If the variable was not set - used the "DefaultLoader" class by default.
loading-module = "com.zooz.kafkas3connector.modules.DefaultLoader"
loading-module = ${?LOADING_MODULE}
play.modules.enabled += ${loading-module}

# Registers all metrics to be scraped by Prometheus
play.modules.enabled += "com.zooz.kafkas3connector.modules.PrometheusModule"

# Telling Play not to apply any filters on the HTTP requests
play.filters.enabled = []

#
# Default values to some of the configuration parameters. Read below for full description of
# each parameter
#
app.id = "s3-connector"
app.buffers_local_dir = "tmp"
app.use_cache = true
app.buffer_size_bytes = 1048576 // 1 MB
app.number_of_flush_workers = 10
app.flush_timeout_seconds = 300
app.microbatch_interval_ms = 10
app.process_loop_timeout_seconds = 300
app.max_flush_retries = 5
app.flush_interval_min = 15
app.shutdown_abort_timeout_seconds = 30
app.skip_corrupted_messages = false
kafka.healthy_lag = 500
kafka.max_poll_records = "1000"
kafka.default_startpoint = "latest"
s3.upload_threads_per_file = 2
s3.multipart_threshold_mb = 15
s3.pattern_path = "date=%s/hour=%s"
s3.region = "us-east-1"
s3.extension = "json"
s3.multipart_threshold_mb = 15
mapper.PCSUrlRoot = "/v1/configurations/%s"
mapper.MerchantStorageRootUrl = "/merchants"
play.http.secret.key = "laksjdhfk@!?KJHDAaskdjhflkja234eASDFa98asfdhlkj23r@**@#$#@4;lasf"

# The port to which Play binds to
play.server.http.port = 9000
play.server.http.port = ${?PORT}

# Global parameters
app {
  # Application id string. This will be used as part of the S3 output path, and also as the
  # Kafka consumer-group name
  id = ${?APP_ID}
  
  # The interval between flushes, in minutes. A flush is when all the messages read from 
  # kafka are copied from the local buffers to the target locations in S3. Default is 15. 
  flush_interval_min = ${?FLUSH_INTERVAL_MINUTES}
  
  # When a know exception is encountered during a buffer flush - the maximum number of allowed
  # retries to flush this buffer. Default is 5.
  max_flush_retries = ${?MAX_FLUSH_RETRIES}
  
  # Optional. Used for testing use cases only. 
  # Similar to "flush_interval_min", but expresses interval in milliseconds instead of seconds. 
  flush_interval_ms = ${?FLUSH_INTERVAL_MS}
  
  # Optional. Local directory where buffer files are temporarily created before they are flushed
  # (copied) to S3. Defaults to "tmp"
  buffers_local_dir = ${?BUFFERS_LOCAL_DIR}
  
  # Optional. Number of parallel workers participating in concurrently flushing buffers to S3.
  # Defaults to 10 
  number_of_flush_workers = ${?NUMBER_OF_FLUSH_WORKERS}
  
  # Optional. Seconds before a flush operation aborts on a timeout exceptions.
  # Defaults to 300 (5 minutes)
  flush_timeout_seconds = ${?FLUSH_TIMEOUT_SECONDS}
  
  # Optional. Compression format to use on the output files. If not set - no compression
  # will be used
  compress_type = ${?COMPRESSION_TYPE}
  
  # Optional. Whether to first write kafka messages into the buffer's memory instead of its 
  # local file. Defaults to "false" 
  use_cache = ${?USE_CACHE}
  
  # Optional. If "use_cache" was set to "true" - the size, in bytes, of memory allocated per each buffer's cache.
  # This can be memory consuming in case many buffers are kept open (for example - if a topic has a large number of
  # partitions, or if messages' timestamp vary greatly). Defaults to 1048576 (1MB)
  buffer_size_bytes = ${?BUFFER_SIZE_BYTES}
  
  # Optional. How often, in milliseconds, to poll Kafka for new messages. Defaults to 10 milliseconds.  
  microbatch_interval_ms = ${?MICROBATCH_INTERVAL_MS}
  
  # Optional. When processing a bulk of new messages - how many seconds to allow before throwing a Timeout
  # exception. This is especially useful to avoid cases when the poll operation hangs due to Kafka connectivity
  # problems. Defaults to 300 (5 minutes)  
  process_loop_timeout_seconds = ${?PROCESS_LOOP_TIMEOUT_SECS}
  
  # Optional. Number of seconds to wait for all Akka actors to shutdown during a service "shutdown abort"
  # event. After this period - service will forcefully shut down. Default is 30. 
  shutdown_abort_timeout_seconds = ${?SHUTDOWN_ABORT_TIMEOUT_SECS}
}

kafka {
  # Kafka broker string, as a list of comma-separated "host:port" couples
  host = ${KAFKA_HOST}
  
  # The kafka topic to read input from
  topic = ${SOURCE_KAFKA_TOPIC}
  
  # Optional. The maximum difference allowed between the topic's latest offset and the
  # offsets of the messages consumed by the service, afterwhich the kafka healthchecks 
  # would start returning as non-healthy. Defaults to 500
  healthy_lag = ${?HEALTHY_KAFKA_LAG}
  
  # Optional. Maximum number of messages to read from Kafka at each poll. Defaults to 1000 
  max_poll_records = ${?max_poll_records}
  
  # Optional. If no committed offsets are found for this APP_ID - whether to start with the "latest" or
  # "earliest" offsets for the topic. Defaults to "latest"
  default_startpoint = ${?DEFAULT_KAFKA_STARTPOINT}
}

s3 {
  # Target bucket to which files are to be flushed
  bucket = ${S3_BUCKET_NAME}
  
  # Optional. If using S3 static credentials - the AWS key to use. If not specified - 
  # Instance-provider credentials are assumed.
  access_key = ${?S3_ACCESS_KEY}
  
  # Optional. If using S3 static credentials - the AWS secret to use. If not specified - 
  # Instance-provider credentials are assumed.
  secret_key = ${?S3_SECRET_KEY}
  
  # Optional. How many threads to use in order to upload a single file to S3. Defaults to 2.
  upload_threads_per_file = ${?S3_UPLOAD_THREADS_PER_FILE}
  
  # Optional. File size, in mega-bytes, after which a multi-part method should be used for its upload.
  # Defaults to 15.
  multipart_threshold_mb = ${?S3_MULTIPART_THRESHOLD_MB}
  
  # Optional. Root directory in s3. If not specified - the output directories will be created directly under
  # the bucket's root
  root_path = ${?S3_ROOT_PATH}
  
  # Optional. By defaults - the output files are placed in subdirectories according to their message's date and hour.
  # this can be customized by providing a different pattern_path and implementing a custom TimePartitioner class.
  # Defaults to "date=%s/hour=%s", where first "%s" is replaced with the date and the second "%s" is replaced with the
  # hour. 
  pattern_path = ${?S3_PATTERN_PATH}
  
  # Optional. File extension for all output files. Defaults to "json"
  extension = ${?S3_EXTENSTION}
  
  # Optional. S3 region. Defaults to "us-east-1"
  region = ${?S3_REGION}
  
  # Optional. A custom s3 endpoint (used by S3 mocks).
  _endpoint = ${?S3_ENDPOINT}

  # Optional. Whether to skip messages read from Kafka which their parsing caused exceptions.
  # Default is "false"
  skip_corrupted_messages = ${?SKIP_CORRUPTED_MESSAGES}
}

# Place here any parameters that your custom mappers may need
mapper {
}

play.http {
  secret.key = ${?PLAY_SECRET_KEY}
}
